\documentclass[a4paper,UTF8]{article}
\usepackage{ctex}
\usepackage[margin=1.25in]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{epsfig}
\usepackage{color}
\usepackage{mdframed}  
\usepackage{lipsum}
\usepackage{graphicx}
\newmdtheoremenv{thm-box}{Theorem}
\newmdtheoremenv{prop-box}{Proposition}
\newmdtheoremenv{def-box}{定义}

\usepackage{listings}
\usepackage{xcolor}
\lstset{
	numbers=left, 
	numberstyle= \tiny, 
	keywordstyle= \color{ blue!70},
	commentstyle= \color{red!50!green!50!blue!50}, 
	frame=shadowbox, % 阴影效果
	rulesepcolor= \color{ red!20!green!20!blue!20} ,
	escapeinside=``, % 英文分号中可写入中文
	xleftmargin=2em,xrightmargin=2em, aboveskip=1em,
	framexleftmargin=2em
} 

\usepackage{booktabs}

\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.5in}
\setlength{\topmargin}{-0.5in}
% \setlength{\textheight}{9.5in}
%%%%%%%%%%%%%%%%%%此处用于设置页眉页脚%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}                                
\usepackage{lastpage}                                           
\usepackage{layout}                                             
\footskip = 10pt 
\pagestyle{fancy}                    % 设置页眉                 
\lhead{2018年春季}                    
\chead{机器学习导论}                                                
% \rhead{第\thepage/\pageref{LastPage}页} 
\rhead{作业二}                                                                                               
\cfoot{\thepage}                                                
\renewcommand{\headrulewidth}{1pt}  			%页眉线宽，设为0可以去页眉线
\setlength{\skip\footins}{0.5cm}    			%脚注与正文的距离           
\renewcommand{\footrulewidth}{0pt}  			%页脚线宽，设为0可以去页脚线

\makeatletter 									%设置双线页眉                                        
\def\headrule{{\if@fancyplain\let\headrulewidth\plainheadrulewidth\fi%
\hrule\@height 1.0pt \@width\headwidth\vskip1pt	%上面线为1pt粗  
\hrule\@height 0.5pt\@width\headwidth  			%下面0.5pt粗            
\vskip-2\headrulewidth\vskip-1pt}      			%两条线的距离1pt        
 \vspace{6mm}}     								%双线与下面正文之间的垂直间距              
\makeatother  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section}
%\usepackage[thmmarks, amsmath, thref]{ntheorem}
\newtheorem{theorem}{Theorem}
\newtheorem*{definition}{Definition}
\newtheorem*{solution}{Solution}
\newtheorem*{prove}{Proof}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\usepackage{multirow}

%--

%--
\begin{document}
\title{机器学习导论\\
作业二}
\author{151220097, 孙旭东, 248381185@qq.com}
\maketitle

\section{[25pts] Multi-Class Logistic Regression}
教材的章节3.3介绍了对数几率回归解决二分类问题的具体做法。假定现在的任务不再是二分类问题，而是多分类问题，其中标记$y\in\{1,2\dots,K\}$。请将对数几率回归算法拓展到该多分类问题。

\begin{enumerate}[(1)]
	\item \textbf{[15pts]} 给出该对率回归模型的“对数似然”(log-likelihood);
	\item \textbf{[10pts]} 计算出该“对数似然”的梯度。
\end{enumerate}

提示1：假设该多分类问题满足如下$K-1$个对数几率，
\begin{eqnarray*}
	\ln\frac{p(y=1|\mathbf{x})}{p(y=K|\mathbf{x})}&=&\mathbf{w}_1^\mathrm{T}\mathbf{x}+b_1\\
	\ln\frac{p(y=2|\mathbf{x})}{p(y=K|\mathbf{x})}&=&\mathbf{w}_2^\mathrm{T}\mathbf{x}+b_2\\
	&\dots&\\
	\ln\frac{p(y={K-1}|\mathbf{x})}{p(y=K|\mathbf{x})}&=&\mathbf{w}_{K-1}^\mathrm{T}\mathbf{x}+b_{K-1}
\end{eqnarray*}

提示2：定义指示函数$\mathbb{I}(\cdot)$，
$$\mathbb{I}(y=j)=
\begin{cases}
1& \text{若$y$等于$j$}\\
0& \text{若$y$不等于$j$}
\end{cases}$$

\begin{solution}

\begin{enumerate}
	
\item
因为
\begin{equation}
	\ln\frac{p(y=i|\mathbf{x})}{p(y=K|\mathbf{x})}=\mathbf{w}_i^\mathrm{T}\mathbf{x}+b_i
\end{equation} 	
所以
\begin{equation}
	\frac{p(y=i|\mathbf{x})}{p(y=K|\mathbf{x})} = e^{\mathbf{w}_i^\mathrm{T}\mathbf{x}+b_i}
\end{equation}
进一步有
\begin{equation}
	\frac{1-p(y=K|\mathbf{x})}{p(y=K|\mathbf{x})} = \sum_{i=1}^{K-1}e^{\mathbf{w}_i^\mathrm{T}\mathbf{x}+b_i}
\end{equation}
所以可得
\begin{equation}
	p(y=K|\mathbf{x}) = \frac{1}{1 + \sum_{i=1}^{K-1}e^{\mathbf{w}_i^\mathrm{T}\mathbf{x}+b_i}}
\end{equation}
由此可得当$k\neq K$
\begin{equation}
	p(y=k|\mathbf{x}) = \frac{e^{\mathbf{w}_k^\mathrm{T}\mathbf{x}+b_k}}{1 + \sum_{i=1}^{K-1}e^{\mathbf{w}_i^\mathrm{T}\mathbf{x}+b_i}}
\end{equation}
综上所述，我们有
\begin{equation}
		p(y=k|\mathbf{x}) = \begin{cases}
		\frac{e^{\mathbf{w}_k^\mathrm{T}\mathbf{x}+b_k}}{1 + \sum_{i=1}^{K-1}e^{\mathbf{w}_i^\mathrm{T}\mathbf{x}+b_i}} & k\neq K;\\
		\frac{1}{1 + \sum_{i=1}^{K-1}e^{\mathbf{w}_i^\mathrm{T}\mathbf{x}+b_i}} & k = K.
		\end{cases}
\end{equation}
我们假设总共有$m$个样例
\begin{equation}
\begin{aligned}
&\ell(\mathbf{w}, \mathbf{b}) \\
=& \sum_{i=1}^{m}\sum_{j=1}^{K}\mathbb{I}(y_i=j)\ln p(y_i=j|\mathbf{x}_i)\\
=& \sum_{i=1}^{m}\sum_{j=1}^{K-1}\mathbb{I}(y_i=j)\ln \frac{e^{\mathbf{w}_j^\mathrm{T}\mathbf{x}_i+b_j}}{1 + \sum_{t=1}^{K-1}e^{\mathbf{w}_t^\mathrm{T}\mathbf{x}_i+b_t}} + \sum_{i=1}^{m}\mathbb{I}(y_i=K)\ln\frac{1}{1 + \sum_{t=1}^{K-1}e^{\mathbf{w}_t^\mathrm{T}\mathbf{x}_i+b_t}}\\
=& \sum_{i=1}^{m}\sum_{j=1}^{K-1}\mathbb{I}(y_i=j)(\mathbf{w}_j^\mathrm{T}\mathbf{x}_i+b_j - \ln (1 + \sum_{t=1}^{K-1}e^{\mathbf{w}_t^\mathrm{T}\mathbf{x}_i+b_t})) - \sum_{i=1}^{m}\mathbb{I}(y_i=K)\ln(1 + \sum_{t=1}^{K-1}e^{\mathbf{w}_t^\mathrm{T}\mathbf{x}_i+b_t})\\
=&\sum_{i=1}^{m}\sum_{j=1}^{K-1}\mathbb{I}(y_i=j)(\mathbf{w}_j^\mathrm{T}\mathbf{x}+b_j) - \sum_{i=1}^{m}\ln(1 + \sum_{t=1}^{K-1}e^{\mathbf{w}_t^\mathrm{T}\mathbf{x}_i+b_t})
\end{aligned}
\end{equation}
再令$\boldsymbol{\beta}_j = (\mathbf{w}_j; b_j)$以及$\hat{\mathbf{x}} = (\mathbf{x}; 1)$，可以有
\begin{equation}
\ell(\boldsymbol{\beta}) = \sum_{i=1}^{m}\sum_{j=1}^{K-1}\mathbb{I}(y_i=j)(\boldsymbol{\beta}_j^\mathrm{T}\hat{\mathbf{x}}_i) - \sum_{i=1}^{m}\ln(1 + \sum_{t=1}^{K-1}e^{\boldsymbol{\beta}_t^\mathrm{T}\hat{\mathbf{x}}_i})
\end{equation}

\item
由上一问可得：
\begin{equation}
\begin{aligned}
\frac{\partial\ell(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}_j} 
=& \sum_{i=1}^{m}\mathbb{I}(y_i=j)\hat{\mathbf{x}}_i - \sum_{i=1}^{m}\frac{e^{\boldsymbol{\beta}_j^\mathrm{T}\hat{\mathbf{x}}_i}}{1 + \sum_{t=1}^{K-1}e^{\boldsymbol{\beta}_t^\mathrm{T}\hat{\mathbf{x}}_i}}\hat{\mathbf{x}}_i\\
=& \sum_{i=1}^{m}(\mathbb{I}(y_i=j) - \frac{e^{\boldsymbol{\beta}_j^\mathrm{T}\hat{\mathbf{x}}_i}}{1 + \sum_{t=1}^{K-1}e^{\boldsymbol{\beta}_t^\mathrm{T}\hat{\mathbf{x}}_i}})\hat{\mathbf{x}}_i\\
=& \sum_{i=1}^{m}(\mathbb{I}(y_i=j) - 	p(y_i=j|\hat{\mathbf{x}}_i))\hat{\mathbf{x}}_i
\end{aligned}
\end{equation} 

\end{enumerate}	
	
\end{solution}
\newpage


\section{[20pts] Linear Discriminant Analysis}
假设有两类数据，正例独立同分布地从高斯分布$\mathcal{N}(\mu_1,\Sigma_1)$采样得到，负例独立同分布地从另一高斯分布$\mathcal{N}(\mu_2,\Sigma_2)$采样得到，其中参数$\mu_1,\Sigma_1$及$\mu_2,\Sigma_2$均已知。现在，我们定义“最优分类”：若对空间中的任意样本点，分别计算已知该样本采样于正例时该样本出现的概率与已知该样本采样于负例时该样本出现的概率后，取概率较大的所采类别作为最终预测的类别输出，则我们说这样的分类方式满足“最优分类”性质。

试证明：当两类数据的分布参数$\Sigma_1=\Sigma_2=\Sigma$时，线性判别分析 (LDA)方法满足“最优分类”性质。（提示：找到满足最优分类性质的分类平面。）
\begin{solution}
	此处用于写解答(中英文均可)
\end{solution}
\newpage



\section{[55+10*pts] Logistic Regression Programming}
在本题中，我们将初步接触机器学习编程，首先我们需要初步了解机器学习编程的主要步骤，然后结合对数几率回归，在UCI数据集上进行实战。机器学习编程的主要步骤可参见\href{http://blog.csdn.net/cqy_chen/article/details/78690975}{博客}。

本次实验选取UCI数据集Page Blocks（\href{http://lamda.nju.edu.cn/ml2018/PS2/PS2_dataset.zip}{下载链接}）。数据集基本信息如表~\ref{data_inf}所示，此数据集特征维度为10维，共有5类样本，并且类别间样本数量不平衡。

\begin{table}[!h]
	\centering
	\caption{Page Blocks数据集中每个类别的样本数量。}\vspace{3mm}
	\label{data_inf}
	\begin{tabular}{l|cccccc}\hline
		标记     & 1    & 2   & 3  & 4  & 5   & total \\ \hline
		训练集   & 4431 & 292 & 25 & 84 & 103 & 4935  \\
		测试集   & 482  & 37  & 3  & 4  & 12  & 538   \\ \hline
	\end{tabular}
\end{table}

对数几率回归（Logistic Regression, LR）是一种常用的分类算法。面对多分类问题，结合处理多分类问题技术，利用常规的LR算法便能解决这类问题。

\begin{enumerate}[(1)]
    \item \textbf{[5pts]} 此次编程作业要求使用Python 3或者MATLAB编写，请将main函数所在文件命名为~LR\_main.py或者LR\_main.m，效果为运行此文件便能完成整个训练过程，并输出测试结果，方便作业批改时直接调用；	
	\item \textbf{[30pts]} 本题要求编程实现如下实验功能：
	\begin{itemize}
		\item \textbf{[10pts]} 根据《机器学习》3.3节，实现LR算法，优化算法可选择梯度下降，亦可选择牛顿法；
		\item \textbf{[10pts]} 根据《机器学习》3.5节，利用“一对其余”（One vs. Rest, OvR）策略对分类LR算法进行改进，处理此多分类任务；
		\item \textbf{[10pts]} 根据《机器学习》3.6节，在训练之前，请使用“过采样”（oversampling）策略进行样本类别平衡；
	\end{itemize}
	
	

	\item \textbf{[20pts]} 实验报告中报告算法的实现过程（能够清晰地体现 (1) 中实验要求，请勿张贴源码），如优化算法选择、相关超参数设置等，并填写表~\ref{exp_performance}，在\url{http://www.tablesgenerator.com/}上能够方便地制作LaTex表格；
	
	\item \textbf{[附加题 10pts]} 尝试其他类别不平衡问题处理策略（尝试方法可以来自《机器学习》也可来自其他参考材料），尽可能提高对少数样本的分类准确率，并在实验报告中给出实验设置、比较结果及参考文献；
\end{enumerate}
\noindent \textbf{[**注意**]} 本次实验除了numpy等数值处理工具包外禁止调用任何开源机器学习工具包，一经发现此实验题分数为0，请将实验所需所有源码文件与作业pdf文件放在同一个目录下，请勿将数据集放在提交目录中。
\newpage
\noindent{\textbf{实验报告.}}
\begin{table}[!h]
	\centering
	\caption{算法在测试数据集上泛化性能测试结果，先报告在每个类别上的查全率和查准率，最后报告在整个测试数据集上的准确率。}\vspace{3mm}
	\label{exp_performance}
	\begin{tabular}{@{}c|cccccc@{}}
		\toprule
		标记     & 1    & 2    & 3    & 4    & 5    & \begin{tabular}[c]{@{}c@{}}准确率\end{tabular} \\ \midrule
		查全率    & 0.92 & 0.92 & 1.00 & 1.00 & 0.67 & \multirow{2}{*}{0.91}                                     \\
		查准率 & 0.99 & 0.80 & 0.33 & 0.44 & 0.26 &  \\ \cmidrule(r){1-7}
	\end{tabular}
\end{table}

\noindent\textbf{实验目的}\\
使用对数几率回归解决多分类问题\\

\noindent\textbf{实验过程}\\
本次程序的实现主要按照以下几个流程：读入数据，过采样，归一化，进行训练，预测数据。其中关键步骤在于过采样和进行训练。\\\\
过采样：\\
过采样的算法采用了smote算法，也就是针对每一个少数类进行过采样。具体的流程为对于少数类中的每一个样例，寻找他们的k近邻，然后随机挑选一个k近邻，在该样例和近邻之间插值，作为插入的新样本。如此重复，直到少数类数量接近其他类。\\\\
训练过程：\\
因为是多分类问题，所以训练的过程主要采用了One vs Rest的思想，每次把一个类作为正例，其余所有类作为反例，按照二分类问题的方法来训练出一组针对正例类的参数，这样针对每一个类别进行一次训练，共计训练出5个模型。\\
在训练的过程中，需要首先确定模型的cost function，我所采用的cost function是
\begin{equation}
J = \frac{1}{m}\sum_{i=1}^{m}(-y_i\log(h(\mathbf{x}_i)) + (1-y_i)\log(1-h(\mathbf{x}_i)))
\end{equation}
其中
\begin{equation}
h(\mathbf{x}) = \frac{1}{1 + \exp(-\boldsymbol{\theta}^\mathrm{T}\mathbf{x})}
\end{equation}
接下来只要对$J$最小化，求得对应的$\boldsymbol{\theta}$，就完成了对这个类的训练。在此基础上，我为$cost$增加了正则项，来避免过拟合的现象。\\
具体的优化算法我采用的是梯度下降，大致流程为首先计算出$J$对于$\theta$的偏导数：
\begin{equation}
\frac{\partial J}{\partial \boldsymbol{\theta}} = \frac{1}{m}\sum_{i=1}^{m}(h(\mathbf{x}_i )- y_i)\mathbf{x}_i
\end{equation}
梯度下降的迭代公式为
\begin{equation}
\boldsymbol{\theta}^k = \boldsymbol{\theta}^{k-1} - \alpha\frac{\partial J}{\partial \boldsymbol{\theta}^{k-1}}
\end{equation}
其中$\alpha$是下降步长。（由于采用了正则化，所以实际的梯度下降公式多了一项关于正则化的，但是总体思路没有区别）\\
训练结束得到最终的$\boldsymbol{\theta}$之后，就可以用$\boldsymbol{\theta}$来对测试集做预测了。\\\\
预测过程：\\
考虑到这是一个多分类问题，所以在进行预测的时候实际上需要得到多个预测值然后进行比较。本题是5类，所以根据上面的训练过程会得到5个对应的$\boldsymbol{\theta}$，记为$\mathbf{T} = (\boldsymbol{\theta}^{(1)}, \boldsymbol{\theta}^{(2)}, ..., \boldsymbol{\theta}^{(5)})$，对于每一个测试样例$(\mathbf{x}; y)$，计算$h_i(\mathbf{x}) = \frac{1}{1 + \exp(-{\boldsymbol{\theta}^{(i)}}^\mathrm{T}\mathbf{x})}$，得到$k=arg \max_i h_i(\mathbf{x})$，那么就预测样例$(\mathbf{x}; y)$属于$k$类。\\\\
归一化：\\
考虑到不同特征之间差异较大，有的特征大小可以达到几千，有的不到1，所以进行了归一化的处理，具体操作为针对每一种特征，计算其均值$\mu$和方差$\sigma$，然后对每一个特征值$t$，构造$t' = (t-\mu)/\sigma$，这样不同的特征的范围就大致相同了。\\\\
超参数的设置：\\
超参数主要包括k近邻个数，梯度下降迭代次数，步长等，设置如下表：
\begin{table}[!htbp]
	\centering
	\caption{超参数设置}
	\label{my-label}
	\begin{tabular}{|l|l|}
		\hline
		$k$(近邻个数)          & 5    \\ \hline
		$\lambda$(正则化参数)    & 0.1  \\ \hline
		$\alpha$(梯度下降步长)     & 5    \\ \hline
		$num\_iters$(梯度下降迭代次数) & 1000 \\ \hline
	\end{tabular}
\end{table}

\noindent\textbf{实验结果}\\
对于训练集的预测准确度达到了0.94，对于测试集的预测准确度到达了0.91，预测的结果综合来看比较乐观。\\\\

\noindent\textbf{实验心得}\\
\begin{itemize}
\item 对于少数类别的处理非常重要，提高少数类别的准确率也很不容易。
\item 选择不同的优化算法会显著影响结果收敛与否，收敛速度。 
\item 如何处理不同特征之间范围差异过大的问题也会影响算法的性能。
\item matlab程序的编写和c++还是有很大不同，想要灵活熟练运用matlab还需要大量的积累和练习。
\end{itemize}

\end{document}