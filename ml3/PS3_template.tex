\documentclass[a4paper,UTF8]{article}
\usepackage{ctex}
\usepackage[margin=1.25in]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{epsfig}
\usepackage{color}
\usepackage{mdframed}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{algorithm}  
\usepackage{algpseudocode}  
\newmdtheoremenv{thm-box}{Theorem}
\newmdtheoremenv{prop-box}{Proposition}
\newmdtheoremenv{def-box}{定义}

\usepackage{listings}
\usepackage{xcolor}
\lstset{
	numbers=left, 
	numberstyle= \tiny, 
	keywordstyle= \color{ blue!70},
	commentstyle= \color{red!50!green!50!blue!50}, 
	frame=shadowbox, % 阴影效果
	rulesepcolor= \color{ red!20!green!20!blue!20} ,
	escapeinside=``, % 英文分号中可写入中文
	xleftmargin=2em,xrightmargin=2em, aboveskip=1em,
	framexleftmargin=2em
} 

\usepackage{booktabs}

\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.5in}
\setlength{\topmargin}{-0.5in}
% \setlength{\textheight}{9.5in}
%%%%%%%%%%%%%%%%%%此处用于设置页眉页脚%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}                                
\usepackage{lastpage}                                           
\usepackage{layout}                                             
\footskip = 10pt 
\pagestyle{fancy}                    % 设置页眉                 
\lhead{2018年春季}                    
\chead{机器学习导论}                                                
% \rhead{第\thepage/\pageref{LastPage}页} 
\rhead{作业三}                                                                                               
\cfoot{\thepage}                                                
\renewcommand{\headrulewidth}{1pt}  			%页眉线宽，设为0可以去页眉线
\setlength{\skip\footins}{0.5cm}    			%脚注与正文的距离           
\renewcommand{\footrulewidth}{0pt}  			%页脚线宽，设为0可以去页脚线

\makeatletter 									%设置双线页眉                                        
\def\headrule{{\if@fancyplain\let\headrulewidth\plainheadrulewidth\fi%
\hrule\@height 1.0pt \@width\headwidth\vskip1pt	%上面线为1pt粗  
\hrule\@height 0.5pt\@width\headwidth  			%下面0.5pt粗            
\vskip-2\headrulewidth\vskip-1pt}      			%两条线的距离1pt        
 \vspace{6mm}}     								%双线与下面正文之间的垂直间距              
\makeatother  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section}
%\usepackage[thmmarks, amsmath, thref]{ntheorem}
\newtheorem{theorem}{Theorem}
\newtheorem*{definition}{Definition}
\newtheorem*{solution}{Solution}
\newtheorem*{prove}{Proof}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\usepackage{multirow}

%--

\tikzset{
	treenode/.style = {shape=rectangle, rounded corners,
		draw, align=center,
		top color=white, bottom color=blue!20},
	root/.style     = {treenode, font=\Large, bottom color=red!30},
	env/.style      = {treenode, font=\ttfamily\normalsize},
	dummy/.style    = {circle,draw}
}

%--
\begin{document}
\title{机器学习导论\\
作业三}
\author{学号, 作者姓名, 邮箱}
\maketitle



\section{[15pts] Decision Tree I}

\begin{enumerate}[ {(}1{)}]
	\item \textbf{[5pts]} 假设一个包含三个布尔属性${X, Y, Z}$的空间，并且目标函数是$f(x,y,z) = x\ \mathbf{XOR}\ z$，其中$\mathbf{XOR}$为异或运算符。令$H$为基于这三个属性的决策树，请问：目标函数$f$可实现吗？如果可实现，画出相应的决策树以证明；如果不可实现，请论证原因；
	
	\item \textbf{[10pts]} 现有如表~\ref{table:ranking}所示数据集：
	
	\begin{table}[!h]
		\centering
		\caption{样例表} \vspace{2mm}\label{table:ranking}
		\begin{tabular}{c c c|c}\hline
			$X$ & $Y$ & $Z$ & $f$ \\
			\hline
			$1$ & $0$  & $1$ &  $1$\\
			$1$ & $1$  & $0$ &  $0$\\
			$0$ & $0$  & $0$ &  $0$\\
			$0$ & $1$  & $1$ &  $1$\\
			$1$ & $0$  & $1$ &  $1$\\
			$0$ & $0$  & $1$ &  $0$\\
			$0$ & $1$  & $1$ &  $1$\\
			$1$ & $1$  & $1$ &  $0$\\
			\hline
		\end{tabular}
	\end{table}
	
	请画出由该数据集生成的决策树。划分属性时要求以信息增益 (information gain)为准则。当信息增益 (information gain)相同时，依据字母顺序选择属性即可。
\end{enumerate}
\begin{solution}
~\\
~\\
~\\
~\\
~\\
~\\
\begin{enumerate}
\item[(1)]目标函数$f$是可以实现的，决策树如下所示：\\\\
\begin{tikzpicture}
[
grow                    = down,
edge from parent/.style = {draw, -latex},
every node/.style       = {font=\footnotesize}
]
\tikzstyle{level 1}=[level distance=20mm,sibling distance=5cm]
\tikzstyle{level 2}=[level distance=20mm,sibling distance=2cm]
\tikzstyle{level 3}=[level distance=20mm,sibling distance=1cm]
\node [root] {x = ?}
child { node [env] {z = ?}
    child { node [env] { f = 0}
        edge from parent node [below] {0} }
    child { node [env] {f = 1}
        edge from parent node [below] {1} }
	edge from parent node [below] {0} }
child { node [env] {z = ?}
    child { node [env] { f = 1}
        edge from parent node [below] {0} }
    child { node [env] {f = 0}
        edge from parent node [below] {1} }
	edge from parent node [below] {1} };
\end{tikzpicture}\\
\item[(2)]
按照信息增益为准则，划分依据为：\\
第一层根结点：\\
如果选择$X$划分：
\begin{equation}
\begin{aligned}
Gain(D, X) =& Ent(D) - \sum_{v=1}^{2}\frac{|D^v|}{|D|}Ent(D^v) \\
=& 1 - 1 = 0 
\end{aligned}
\end{equation}
如果选择$Y$划分：
\begin{equation}
Gain(D, Y) = 1 - 1 = 0 
\end{equation}
如果选择$Z$划分：
\begin{equation}
Gain(D, Y) = 1 - \frac{3}{4}\times 0.918 = 0.6885 
\end{equation}
所以选择$Z$划分。\\
第二层第一个结点都是同一类，现在考虑第二个结点的划分：\\
如果选择$X$划分：
\begin{equation}
Gain(D, X) = 0.918 - 0.918 = 0
\end{equation}
如果选择$Y$划分：
\begin{equation}
Gain(D, Y)  = 0.918 - 0.918 = 0
\end{equation}
所以按照字母顺序选择$X$划分。\\
根据数据集生成的决策树如下：\\\\
\begin{tikzpicture}
[
grow                    = down,
edge from parent/.style = {draw, -latex},
every node/.style       = {font=\footnotesize}
]
\tikzstyle{level 1}=[level distance=20mm,sibling distance=9cm]
\tikzstyle{level 2}=[level distance=20mm,sibling distance=4cm]
\tikzstyle{level 3}=[level distance=20mm,sibling distance=2cm]
\node [root] {Z = ?}
child { node [env] {f = 0}
	edge from parent node [below] {0} }
child { node [env] {X = ?}
	child { node [env] { Y = ?}
		child {node [env] {f=0}
			edge from parent node [below] {0} }
		child {node [env] {f=1}
			edge from parent node [below] {1} }
		edge from parent node [below] {0} }
	child { node [env] { Y = ?}
		child {node [env] {f=1}
			edge from parent node [below] {0} }
		child {node [env] {f=0}
			edge from parent node [below] {1} }
		edge from parent node [below] {1} }
	edge from parent node [below] {1} };
\end{tikzpicture}\\
\end{enumerate}
\end{solution}
\newpage



 \section{[20pts] Decision Tree II}
 考虑如下矩阵：
 $$
 \begin{bmatrix}
 4 & 6 & 9 & 1 & 7 & 5 \\
 1 & 6 & 5 & 2 & 3 & 4
 \end{bmatrix}^T
 $$

 该矩阵代表了$6$个样本数据，每个样本都包含$2$个特征$f_1$和$f_2$。这$6$个样本数据对应的标签如下：
 $$
 \begin{bmatrix}
 1 & 0 & 1 & 0 & 1 & 0
 \end{bmatrix}^T
 $$

 在这个问题中，我们要构造一个深度为$2$的树进行分类任务。

 \begin{enumerate}[ {(}1{)}]
 	\item \textbf{[5pts]} 请计算根结点 (root) 的熵值 (entropy)；

 	\item \textbf{[10pts]} 请给出第一次划分的规则，例如$f_1 \geq 4, f_2 \geq 3$。对于第一次划分后产生的两个结点，请给出下一次划分的规则；

 	提示：可以直观判断，不必计算熵。

 	\item \textbf{[5pts]} 现在回到根结点 (root)，并且假设我们是建树的新手。是否存在一种划分使得根结点 (root) 的信息增益 (information gain) 为$0$?
 \end{enumerate}

 \begin{solution}
 ~\\
 \begin{enumerate}
 \item [(1)]
 根结点的熵值为
 \begin{equation}
 Ent(D) = -(\frac{3}{6}\log_2\frac{3}{6} + \frac{3}{6}\log_2\frac{3}{6}) = 1.000
 \end{equation}
 \item [(2)]
 规则如图1所示。第一次划分的规则为$f_1\leq 6$。不满足$f_1\leq 6$的结点的所有样本的标签都是$1$，所以这一部分不需要再次划分。第一次划分之后，满足$f_1\leq 6$的节点的第二次划分的规则为$f_2\leq 1$，其中满足$f_2\leq 1$的结点的样本的标签都都是$1$，不满足的都是$0$，所以不需要再次划分。
 \begin{figure}[!h]
 	\centering   
 	\includegraphics[scale=0.5]{coordinate1.png}  
 	\caption{划分规则} 
 	\label{coordinate1}
 \end{figure}
 \item [(3)]
 规则如图2所示。按照$f_2\leq 2$来划分，将$D$分成了$D^1$和$D^2$两个部分。其中$D^1$包含第一个和第四个样例。由此可得
 \begin{equation}
 Ent(D^1) =  -(\frac{1}{2}\log_2\frac{1}{2} + \frac{1}{2}\log_2\frac{1}{2}) = 1.000
 \end{equation}
 \begin{equation}
 Ent(D^2) = -(\frac{2}{4}\log_2\frac{2}{4} + \frac{2}{4}\log_2\frac{2}{4}) = 1.000
 \end{equation}
 所以可得信息增益为
 \begin{equation}
 \begin{aligned}
 Gain(D, f_2\leq 2) &= Ent(D) - \sum_{v=1}^{2}\frac{|D^v|}{|D|}Ent(D^v)\\
 &= 1 - (\frac{2}{6}\times 1 + \frac{4}{6}\times 1)\\
 &= 0
 \end{aligned}
 \end{equation}
 \begin{figure}[!h]
 	\centering   
 	\includegraphics[scale=0.5]{coordinate2.png}  
 	\caption{划分规则} 
 	\label{coordinate2}
 \end{figure}
 \end{enumerate}
 \end{solution}
\newpage

\section{[25pts] Universal Approximator}
已知函数$f:[-1, 1]^n \mapsto [-1, 1]$满足$\rho$-Lipschiz性质。 给定误差$\epsilon > 0$，请构造一个激活函数为\mbox{ sgn($\mathbf{x}$) }的神经网络$ \mathcal{N}:[-1,1]^n \mapsto [-1,1] $，使得对于任意的输入样本$ \mathbf{x} \in [-1,1]^n $，有$|f(\mathbf{x}) - \mathcal{N}(\mathbf{x})| \leq \epsilon$。\\
(Lipschitz条件参见\href{https://en.wikipedia.org/wiki/Lipschitz_continuity}{Wikipedia}，其中\mbox{ sgn($\mathbf{x}$) }的定义参见《机器学习》第98页。)

\begin{enumerate}[ {(}1{)}]
	\item \textbf{[5pts]} 请画出构造的神经网络$\mathcal{N}$的示意图；
	
	\item \textbf{[10pts]} 请对构造的神经网络进行简要的说明(写清每一层的线性组合形式，也就是结点间的连接方式和对应的权重)；
	
	\item \textbf{[10pts]} 证明自己构造的神经网络的拟合误差满足要求。
\end{enumerate}


\begin{solution}

\begin{enumerate}[ {(}1{)}]
\item 神经网络$\mathcal{N}$的示意图如下所示（因为空间有限，所以省略了部分结点和边，以及所有权值为0的边均在图中省略）：
\begin{figure}[!h]
	\centering   
	\includegraphics[scale=0.3]{net.png}  
	\caption{神经网络$\mathcal{N}$} 
	\label{net}
\end{figure}	
	
\item 神经网络说明：\\
\textbf{$\mathcal{N}$的结构：}\\
神经网络分为$4$层，其中包括输入层，输出层和两个隐层。其中输入层有$n$个输入单元（每个单元对应了一个$\mathbf{x}$的维度的数值），第一个隐层（以后称为B层）有$2mn$个单元，第二个隐层（以后称为T层）有$m^n$个单元，输出层只有一个单元也就是整个神经网络的输出值。\\
\textbf{$\mathcal{N}$的符号表示：}
见下表：
\begin{table}[!h]
	\centering
	\caption{符号表}
	\label{my-label}
	\begin{tabular}{|l|l|}
		\hline
	$x_i$	& 输入单元接受的输入值，也就是$ \mathbf{x} $向量的各个维度的数值 \\ \hline
	$b_{i,j}^+, b_{i,j}^-$	& B层的单元，注意到$b_{i,j}^+$和$b_{i,j}^-$是成对出现的 \\ \hline
	$\alpha_{i,j}^+,\alpha_{i,j}^-$	& 分别是$b_{i,j}^+$和$b_{i,j}^-$的输入 \\ \hline
	$\beta_{i,j}^+,\beta_{i,j}^-$	& 分别是$b_{i,j}^+$和$b_{i,j}^-$的输出 \\ \hline
	$\theta_{i,j}^+,\theta_{i,j}^-$	& 分别是$b_{i,j}^+$和$b_{i,j}^-$的阈值 \\ \hline
	$t_i$	& T层的单元 \\ \hline
	$\gamma_i$	& $t_i$的输入 \\ \hline
	$\delta_i$	& $t_i$的输出 \\ \hline
	$\lambda_i$	& $t_i$的阈值 \\ \hline
	$z$	& 输出层的单元的输出值 \\ \hline
	$K$	& 函数$f$的Lipschiz常数 \\ \hline
	$m$	& 神经网络的超参数，用于控制B层的神经元的数目\\ \hline
	$\sigma$	& $\sigma>0$，加在$\theta_{i,m}^-$上，确保$x_i=1$时也能正确处理\\ \hline
	\end{tabular}
\end{table}\\
\textbf{$\mathcal{N}$的各层的线性组合形式：}\\
输入层神经元数目：$n$，即$x_1, x_2,...,x_n$\\
B层神经元数目：$2mn$，即$b_{1,1}^*,...,b_{1,m}^*,...,b_{n,1}^*,...,b_{n,m}^*\ ,\ \ *\in \{+,-\}$\\
T层神经元数目：$m^n$，即$t_1,t_2,...,t_{m^n}$\\
输出层神经元数目：$1$，即$z$\\
从输入层到B层的传播：
\begin{equation}
\alpha_{i,j}^* = x_i\ ,\ \ *\in \{+,-\}
\end{equation}
B层的神经元计算：
\begin{equation}
\theta_{i,j}^+ = -1 + (j-1)\frac{2}{m}\\
\end{equation}
\begin{equation}
\theta_{i,j}^- =
\begin{cases}
-1 + j\frac{2}{m} & 1\leq i\leq m-1;\\
-1 + j\frac{2}{m} + \sigma & i = m;
\end{cases}
\end{equation}
\begin{equation}
\beta_{i,j}^* = sgn(\alpha_{i,j}^* - \theta_{i,j}^*)\ ,\ \ *\in \{+, -\}
\end{equation}
从B层到T层，映射较为复杂：B层的神经元$b_{1, index_1}^+,b_{2,index_2}^+,...,b_{n,index_n}^+$的输出乘以权重$1$以及$b_{1, index_1}^-,b_{2,index_2}^-,...,b_{n,index_n}^-$的输出乘以权重$-1$之后作为$t_s$的输入值，其中$s= 1+\sum_{i=1}^{n}(index_i-1)m^{(i-1)} $，所以为了根据$s$得到$index_i$，给出以下算法：
\begin{algorithm}[h]  
	\caption{IndexTranslate}  
	\begin{algorithmic}[1]  
		\Require  
		$s$ : T层的神经元编号 
		\Ensure  
		$(index_1,index_2,...,index_n)$ : 输入到$t_s$的B层神经元的编号
		\State $s = s-1$ 
		\For{i from 1 to n}  
		\State $index_i = s \mod m^i$
		\State $s = s - index_i$
		\State $index_i = index_i + 1$
		\EndFor
		\label{code:recentEnd}  
	\end{algorithmic}  
\end{algorithm}\\
现在给出从B层到T层的传播：
\begin{equation}
(index_1, index_2,...,index_n) = IndexTranslate(s)
\end{equation}
\begin{equation}
\gamma_s = \sum_{i=1}^{n}\beta_{i,index_i}^+ - \sum_{i=1}^{n}\beta_{i,index_i}^-
\end{equation}
T层的神经元计算：\\
\begin{equation}
\lambda_s = n - \frac{1}{2}
\end{equation}
\begin{equation}
\delta_s = sgn(\gamma_s - \lambda_s)
\end{equation}
从T层到输出层的边的权值其实是函数$f$在某些点的值，下面给出算法通过$t_s$的编号$s$来计算$\delta_s$对应的权值：
\begin{algorithm}[h]  
	\caption{GetWeightFromIndex}  
	\begin{algorithmic}[1]  
		\Require  
		$s$ : T层的神经元编号  
		\Ensure  
		$w_s$ : 从T层到输出层的边中$\delta_s$对应的边的权重
		\State $(index_1, index_2,...,index_n) = IndexTranslate(s)$ 
		\For{i from 1 to n}  
		\State $xt_i = -1 + (index_i - \frac{1}{2})\frac{2}{m}$
		\EndFor
		\State $\mathbf{xt} = [xt_1, xt_2,...,xt_n] $
		\State $w_s = f(\mathbf{xt})$
		\label{code:recentEnd}  
	\end{algorithmic}  
\end{algorithm}\\
通过上述算法计算每一个$\delta_i$的权值$w_i = GetWeightFromIndex(i)$。\\
输出层的最终输出：
\begin{equation}
z = \sum_{i=1}^{m^n}w_i\delta_i
\end{equation}
\item 证明：\\
在进行具体的证明之前，首先对神经网络$\mathcal{N}$的设计思路进行说明：由于需要逼近的函数$f$具有Lipschiz性质，即：
\begin{equation}
|f(\mathbf{x}) - f(\mathbf{y})| \leq K||\mathbf{x} - \mathbf{y}||
\end{equation}
所以$\mathcal{N}$的大致思想就是将$f$的定义域分割成很多个足够小的子空间，根据Lipschiz性质，子空间内的任意两个点$\mathbf{x}$和$\mathbf{y}$对应的函数值$f(\mathbf{x})$和$f(\mathbf{y})$相差会有一个上界即$K||\mathbf{x} - \mathbf{y}||$。只要确定了分割的子空间的大小，那么$||\mathbf{x} - \mathbf{y}||$也会随之确定。所以只要确保将定义域分割的足够细使得$K||\mathbf{x} - \mathbf{y}|| \leq \epsilon$即可得到符合题意的神经网络。考虑到有限维向量范数的等价性，为了方便之后的证明，在这里取2-范数（即便取其他范数，我们的结论仍然不会改变，这个将在之后证明），即：
\begin{equation}
|f(\mathbf{x}) - f(\mathbf{y})| \leq K||\mathbf{x} - \mathbf{y}||_2
\end{equation}
设置$m=\frac{k\sqrt{n}}{\epsilon}$，然后将定义域空间$[-1,1]^n$等分成$m^n$份，具体的：对于维度$i$，将其等分为$m$份，得到$m$个区间$[-1, -1+\frac{2}{m}),\ [-1+\frac{2}{m},-1+2\frac{2}{m}),\ ...,\ [-1 + (m-1)\frac{2}{m}, 1]$，每个区间的长度为$\frac{2}{m}$（注意到最后一个区间两边都是闭的）。现在定义$D_{i,j}$表示第$i$维的第$j$个区间即$D_{i,j} = [-1+(j-1)\frac{2}{m},-1+j\frac{2}{m})$，在此基础上再定义子空间$S(j_1,j_2,...,j_n) = D_{1,j_1} \times D_{2,j_2} \times ... \times D_{n, j_n}$。\\
下证：对于对于任意的输入样本$ \mathbf{x} \in [-1,1]^n $，有$|f(\mathbf{x}) - \mathcal{N}(\mathbf{x})| \leq \epsilon$。\\
case 1：$ \mathbf{x} \in [-1,1)^n $\\
考虑$\mathbf{x}$的每一个维度的值$(x_1,...,x_n)$，不失一般性的假设$\mathbf{x} \in S(index_1, index_2,..,index_n)$，即：
\begin{equation}
\begin{aligned}
x_1 \in [-1+(index_1 &- 1)\frac{2}{m},-1 + index_1\frac{2}{m}) \\
x_2 \in [-1+(index_2 &- 1)\frac{2}{m},-1 + index_2\frac{2}{m}) \\
&......\\
x_n \in [-1+(index_n &- 1)\frac{2}{m},-1 + index_n\frac{2}{m})
\end{aligned}
\end{equation}
现在观察到任意$i$，都有$x_i \geq -1+(index_i - 1)\frac{2}{m}$以及$x_i < -1+index_i\frac{2}{m}$，结合$(3.3)$可得：
\begin{equation}
\begin{aligned}
&\beta_{i,j}^* = 1\ \ \ for\ *\in \{+,-\}\ and\ j < index_i\\
&\beta_{i,index_i}^+ = 1\\
&\beta_{i,index_i}^- = 0\\
&\beta_{i,j}^* = 0\ \ \ for\ *\in \{+,-\}\ and\ j > index_i
\end{aligned}
\end{equation}
现在定义$s= 1+\sum_{i=1}^{n}(index_i-1)m^{(i-1)}$，又根据$(3.6)$可得：
\begin{equation}
\gamma_s = \sum_{i=1}^{n}\beta_{i,index_i}^+ - \sum_{i=1}^{n}\beta_{i,index_i}^- = n
\end{equation}
现在再考虑$s'$且$s'\neq s$，定义$(index_1', index_2',...,index_n') = IndexTranslate(s')$，由于$s'\neq s$，所以一定存在$i$使得$index_i\neq index_i'$，那么根据$(3.13)$可得$\beta_{i,index_i'}^+ - \beta_{i,index_i'}^- = 0$，即：
\begin{equation}
\gamma_{s'} \leq n-1,\ \ \ for\ s'\neq s
\end{equation}
又因为$\lambda_s = \lambda_{s'} = n - \frac{1}{2}$，所以根据$(3.8)$可得：
\begin{equation}
\begin{aligned}
&\delta_s = sgn(\gamma_s - \lambda_s) = 1\\
&\delta_{s'} = sgn(\gamma_{s'} - \lambda_{s'}) = 0\ \ \ for\ s'\neq s
\end{aligned}
\end{equation}
结合上式再根据$(3.9)$可得：
\begin{equation}
z = \sum_{i=1}^{m^n}w_i\delta_i = w_s\delta_s = GetWeightFromIndex(s)
\end{equation}
根据$Algorithn\ 2$可知$GetWeightFromIndex(s) = w_s = f(\mathbf{xt})$其中$\mathbf{xt} = [xt_1,...xt_n]$而且$xt_i=  -1 + (index_i - \frac{1}{2})\frac{2}{m}$
\end{enumerate}


\end{solution}
\newpage

\section{[40pts] Neural Network in Practice}
通过《机器学习》课本第5章的学习，相信大家已经对神经网络有了初步的理解。深度神经网络在某些现实机器学习问题，如图像、自然语言处理等表现优异。本次作业旨在引导大家学习使用一种深度神经网络工具，快速搭建、训练深度神经网络，完成分类任务。

我们选取PyTorch为本次实验的深度神经网络工具，有了基础工具，我们就能如同搭积木一样构建深度神经网络。\href{http://pytorch.org/}{PyTorch}是Facebook开发的一种开源深度学习框架，有安装方便、文档齐全、构架方便、训练效率高等特点。本次作业的首要任务就是安装PyTorch。

目前PyTorch仅支持Linux和MacOS操作系统，所以Window用户需要装一个Linux虚拟机或者直接安装Linux系统。PyTorch安装很方便，只需要在其主页中的Get Start一栏选择对应的环境设置，便能够一键安装。有GPU的同学也可以尝试安装GPU版本的PyTorch。为保证此次作业的公平性，只要求使用CPU进行网络训练，当然有条件的同学也可以尝试使用GPU进行训练。在批改作业时，助教会提供Python 2.7、3.5、3.6三种环境进行实验验证。

我们选取CIFAR10作为本次作业的训练任务。\href{https://en.wikipedia.org/wiki/CIFAR-10}{CIFAR10}是一个经典的图片分类数据集，数据集中总共有60000张32$\times$32的彩色图片，总共有10类，每类6000张图片，其中50000张图片构成训练集，10000张图片构成测试集。PyTorch通过torchvision给用户提供了获取CIFAR10的方法，详细信息可见\href{http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html}{PyTorch的教程}。此外关于CIFAR10分类准确率排行可见此\href{http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html}{链接}。

下面我们将尝试使用PyTorch来解决实际问题：

\begin{enumerate}[(1)]
	\item \textbf{[15pts]} 首先我们跟随PyTorch的教程，用一个简单的卷积神经网络（Convolutional Neural Network, CNN），完成CIFAR10上的分类任务，具体要求如下：
	
	\begin{itemize}
		\item \textbf{[7pts]} 在代码实现之前，大家可能需要对CNN网络进行一定的了解，请大家自行查阅资料（PyTorch的教程中也有部分介绍CNN网络），并在实验报告中给出对CNN的见解：主要回答什么是卷积层，什么是Pooling层，以及两者的作用分别是什么；
		\item \textbf{[8pts]} 接下来就是具体的代码实现和训练。教程会手把手教你完成一次训练过程，其中使用SGD作为优化方法，请同学们自行调整epoch的大小和学习率，完成此次训练。另外，请在实验报告中给出必要的参数设置，以及训练结果如最终的loss、在测试集上的准确率等；
	\end{itemize}
	\item \textbf{[20pts]} 显然，这样一个简单的网络在CIFAR10上并不能取得令人满意的结果，我们需要选取一个更为复杂的网络来提升训练效果。在此小题中，我们选取了CIFAR10准确率排行榜上排名第二的结构，具体参见\href{https://arxiv.org/pdf/1412.6806.pdf}{论文链接}。为了方便大家实现，我们直接给出了网络结构如图\ref{network_structure}所示。请大家搭建完成此网络结构，并选择Adam为优化器，自行调整相关参数完成训练和预测，实验结果报告内容同第（1）小题；
	\begin{figure}[!h]
		\centering   
		\includegraphics[width=0.99\textwidth, height=0.15\textwidth]{nn_structure.png}  
		\caption{待实现网络结构} 
		\label{network_structure}
	\end{figure}
	\item \textbf{[5pts]} 通过上一题实验我们可以发现，即使使用现成的网络结构也不一定能达到与其相同的训练效果。请大家分析其中的原因，并谈谈本次实验的感想，以及对深度学习调参的体会。
\end{enumerate}

\noindent{\textbf{实验报告.}}

\begin{enumerate}
\item [(1)]
~\\
\textbf{对CNN的见解}：卷积神经网络是一种非常强大的，适合用于图像，视频识别以及自然语言处理的神经网络的。卷积神经网络本身就是一种特殊的神经网络，其训练的流程为：输入层接受训练集数据，通过隐层一层层传递到输出层，输出与真实标签进行比较，得到损失函数，然后再通过反向传播来调整隐层的参数，进而降低损失函数，来使得预测结果逼近与真实标签。CNN中比较特殊的隐层结构是卷积层和pooling层，其中卷积层主要承担了对特定模式的识别工作，pooling层主要起到了采样的作用。\\
\textbf{卷积层的概念}：是卷积神经网络的核心，承担了卷积神经网络大部分的工作量。\\
\textbf{卷积层的作用}：卷积层的作用主要在于提取特征，这个操作类似于信号处理中的滤波，和人类大脑认知世界也有几分相似。这个操作的实现用到了卷积的方法。具体的，如果输入的数据大小为$H\times W\times D$(此处先假设batch size为1，其中D表示channel数量)，那么卷积层就会有诸多大小为$H'\times W'\times D$的filter(其中$H' < H$而且$W' < W$)。通过将一个filter“覆盖”在输入数据上，会得到输入数据上的一个和filter相同大小的数据块，把数据块和filter对应位置的元素相乘然后求和，可以得到一个标量值，把filter“覆盖”到数据的不同位置可以得到很多标量值，将这些标量值按照顺序排列起来可以得到一个activation map，每一个filter针对每一个输入数据都会得到一个activation map，把多个filter对应的activation map堆叠在一起就可以得到下一个隐层的输入数据。每一个卷积层得到的activation map其实就是对某种特征的提取，如果map中某个元素值很大，就说明这个元素对应的输入数据的特定位置有可能存在某种特征。所以activation map实际上可以反应出特征在输入数据集中的分布。卷积层可以识别的特征多种多样的，一般第一卷积层只能识别一些简单的特征，例如边缘，曲线，角，更多层的卷积层可以识别到更加复杂的特征。\\
\textbf{Pooling层的概念}：pooling层也是卷积神经网络中一个重要的结构，他的主要功能在于对数据进行采样。\\
\textbf{Pooling层的作用}：总体来讲pooling层的作用在于减少数据大小，减少参数，降低运算量，减低训练时间，防止过拟合，提高泛化能力，同时还可以保持特征的不变形（包括平移，旋转，尺度等方面）。具体的，pooling层就是按照一定的比例对输入数据进行采样，常见的有max-pooling和average-pooling。常见的做法是将数据分成$2\times 2$的数据块然后从每个数据块中选择一个最大值（或者平均值）来代替整个数据块。其中max-pooling采样操作还能够保持输入数据的特征不变型，比如说某个数据块中出现了某个特征，反映在输入数据上就是某个元素的值非常大。当我们在对这个数据块采样的时候，为了在下一层维持这个特征的表现，我们选择了最大的数值作为采样后的结果，这背后蕴含了即便数据缩小了，我们仍然尽可能保持原有的特征这一思想，所以实践中往往max-pooling表现会比较好。实践中会在卷积层之间周期性的插入pooling层来提升整体的效果。
	
\item [(2)]
~\\
	
\end{enumerate}

\end{document}